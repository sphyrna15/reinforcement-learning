#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sat Apr 11 09:51:54 2020

@author: v1per
"""


""" Q-learning and Deep-Q-learning from HansOn ML book """

import numpy as np
import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt

""" Markov Decision Process and Q-value optimization """

# We have 3 possible States: s0, s1, s2
possible_actions = np.array([[0,1,2], [0,2], [1]])
transition_probabilities = np.array([ # shape [s,a,s']
                                     [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],
                                     [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]],
                                     [None, [0.8, 0.1, 0.1], None]])

rewards = np.array([# shape [s,a,s']
                    [[+10, 0, 0], [0, 0, 0], [0, 0, 0]],
                    [[0, 0, 0], [0, 0, 0], [0, 0, -50]],
                    [[0, 0, 0], [+40, 0, 0], [0, 0, 0]]])

Q_values = np.full((3,3), -np.inf)

for state, action in enumerate(possible_actions):
    Q_values[state, action] = 0.0 

gamma = 0.90
history1 = []

for interation in range(50):
    
    Q_prev = Q_values.copy()
    history1.append(Q_prev)
    for s in range(3):
        for a in possible_actions[s]:
            
            Q_values[s, a] = np.sum([transition_probabilities[s][a][sp] * 
                                     (rewards[s][a][sp] + gamma*np.max(Q_prev[sp]))
                                    for sp in range(3)])
history1 = np.array(history1)

print("Q values found using Bellman Optimality Equation are: ")
print(Q_values)


""" Simple Q-learning """

def step(state, action):
    
    probs = transition_probabilities[state][action]
    next_state = np.random.choice([0, 1, 2], p=probs)
    reward = rewards[state][action][next_state]
    
    return next_state, reward

def exploration_policy(state):
    return np.random.choice(possible_actions[state])

Q_values2 = np.full((3, 3), -np.inf)
for state, actions in enumerate(possible_actions):
    Q_values2[state][actions] = 0

alpha0 = 0.05 # initial learning rate
decay = 0.005 # learning rate decay
gamma = 0.90 # discount factor
state = 0 # initial state
history2 = [] # Not shown in the book

for iteration in range(30000):
    
    history2.append(Q_values2.copy()) # Not shown
    action = exploration_policy(state)
    next_state, reward = step(state, action)
    next_value = np.max(Q_values2[next_state]) # greedy policy at the next step
    alpha = alpha0 / (1 + iteration * decay)
    Q_values2[state, action] *= 1 - alpha
    Q_values2[state, action] += alpha * (reward + gamma * next_value)
    state = next_state

history2 = np.array(history2) # Not shown

print("Q values found using Q-learning algorithm are: ")
print(Q_values2)

true_Q_value = history1[-1, 0, 0]

fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharey=True)
axes[0].set_ylabel("Q-Value$(s_0, a_0)$", fontsize=14)
axes[0].set_title("Q-Value Iteration", fontsize=14)
axes[1].set_title("Q-Learning", fontsize=14)
axes[1].set_ylabel("Q-Value$(s_0, a_0)$", fontsize=14)

for ax, width, history in zip(axes, (50, 30000), (history1, history2)):
    
    ax.plot([0, width], [true_Q_value, true_Q_value], "k--")
    ax.plot(np.arange(width), history[:, 0, 0], "b-", linewidth=2)
    ax.set_xlabel("Iterations", fontsize=14)
    ax.axis([0, width, 0, 24])
    


""" Deep Q-learning algorithm """
import gym
keras.backend.clear_session()
tf.random.set_seed(42)
np.random.seed(42)


env = gym.make("CartPole-v0")
shape = env.observation_space.shape
outputs = env.action_space.n

model = keras.models.Sequential([
    keras.layers.Dense(50, activation = 'elu', input_shape = shape),
    keras.layers.Dense(33, activation = 'elu'),
    keras.layers.Dense(outputs)])

def epsilon_greedy(state, epsilon = 0):
    if np.random.rand() < epsilon:
        return np.random.randint(2)
    else:
        Q_values = model.predict(state[np.newaxis])
        return np.argmax(Q_values[0])
    
from collections import deque

replay_memory = deque(maxlen=2000)

def sample_experiences(batch_size):
    
    indices = np.random.randint(len(replay_memory), size=batch_size)
    batch = [replay_memory[index] for index in indices]
    states, actions, rewards, next_states, dones = [
        np.array([experience[field_index] for experience in batch])
        for field_index in range(5)]
    
    return states, actions, rewards, next_states, dones

def play_one_step(env, state, epsilon):
    
    action = epsilon_greedy(state, epsilon)
    next_state, reward, done, info = env.step(action)
    replay_memory.append((state, action, reward, next_state, done))
    
    return next_state, reward, done, info

batch_size = 32
discount_rate = 0.95
optimizer = keras.optimizers.Adam(lr=1e-3)
loss_fn = keras.losses.mean_squared_error

def training_step(batch_size):
    
    experiences = sample_experiences(batch_size)
    states, actions, rewards, next_states, dones = experiences
    next_Q_values = model.predict(next_states)
    max_next_Q_values = np.max(next_Q_values, axis=1)
    target_Q_values = (rewards +
                       (1 - dones) * discount_rate * max_next_Q_values)
    target_Q_values = target_Q_values.reshape(-1, 1)
    mask = tf.one_hot(actions, outputs)
    
    with tf.GradientTape() as tape:
        all_Q_values = model(states)
        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)
        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))
    
    grads = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(grads, model.trainable_variables))

env.seed(42)
np.random.seed(42)
tf.random.set_seed(42)

rewards = [] 
best_score = 0

for episode in range(600):
    obs = env.reset()    
    for step in range(200):
        epsilon = max(1 - episode / 500, 0.01)
        obs, reward, done, info = play_one_step(env, obs, epsilon)
        if done:
            break
    rewards.append(step) # Not shown in the book
    if step > best_score: # Not shown
        best_weights = model.get_weights() # Not shown
        best_score = step # Not shown
    print("\rEpisode: {}, Steps: {}, eps: {:.3f}".format(episode, step + 1, epsilon), end="") # Not shown
    if episode > 50:
        training_step(batch_size)

model.set_weights(best_weights)


plt.figure()
plt.plot(rewards)
plt.xlabel("Episode", fontsize=14)
plt.ylabel("Sum of rewards", fontsize=14)
plt.show()

env.seed(42)
state = env.reset()

frames = []

import matplotlib.animation as animation

def update_scene(num, frames, patch):
    patch.set_data(frames[num])
    return patch,

def plot_animation(frames, repeat=False, interval=40):
    fig = plt.figure()
    patch = plt.imshow(frames[0])
    plt.axis('off')
    anim = animation.FuncAnimation(
        fig, update_scene, fargs=(frames, patch),
        frames=len(frames), repeat=repeat, interval=interval)
    plt.close()
    return anim

for step in range(200):
    action = epsilon_greedy(state)
    state, reward, done, info = env.step(action)
    if done:
        break
    img = env.render(mode="rgb_array")
    frames.append(img)
    
plot_animation(frames)
    

